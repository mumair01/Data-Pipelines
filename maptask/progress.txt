DATE: 5/9/22
- Started implementing the pipeline
- Added ability to download the maptask corpus including the audio.
- Added ability to separate the left and right tracks of the downloaded audio.
- Added ability to use opensmile binaries to extract genome features


DATE: 5/10/22
- Audio download shell script moved to a shell scripts folder to separate scripts.
- Tested method to extract gemap features as expected in the original code.
    - Now uses downloaded opensmile binary from: https://github.com/audeering/opensmile/releases
    - Binary used to extract audio features using opensmile config files by Skantze.

DATE: 5/11/22
- Implementing the get vocabulary method based on the existing implementation.
- There can be another class called MapTaskProcessor that is specific to
    a specific MapTask pipeline.
- There can be a general pipeline class that can manage the Threads etc.
- Notes for when refactoring:
    - Potential classes:
        1. MapTask class --> Responsible for downloading and parsing the corpus.
    - Features to add when refactoring:
        1. Method for obtaining the files in different directories.
        2. Method for obtaining all files and / or sub-dirs in a dir.
        3. General methods for parsing the downloaded data.
        4. Method for collecting files based on common filenames.
        5. Method to extract features given an audio file.
        6. Methods to write to different file formats i.e., hd5, pickle, json etc.
        7. Methods for parsing specific xml files in the original MapTask data.

- IMPORTANT: Added the pipeline up to get average word annotations - choosing not to move
forward because a lot of code is being re-used and it is becoming difficult
to understand.

- Notes about the MapTask corpus
    - f and g stand for "follower" and "giver" for the participants in the dialogue.
    - Left channel is the follower and right is the giver.
    - For the maptask filename:
        1. qx --> x is the quad number.
        2. nc or ec --> nc is no eye contact and ec is eye contact.
        3. xxxxY --> Y is the dialogue number.


DATE: 5/12/22
- Building the refactored pipeline:
    1. Copied the Threads class from GaiBot to multithread but thinking to
        use multithreading package instead.
    2. Creating a separate script that extract audio features using opensmile
        - This can be reused later.

DATE: 5/13/22
- Picking up and continuing the work outlined for 5/12/22.
- Created a refactored and somewhat more modular pipeline.
- Next step is to create POC notebooks to investigate what the features actually mean.
- Determining the exact features produces by opensmile:
    - NOTE: This has been done in the opensmile_tests directory!
    - Downloaded, installed, and added opensmile to default .zshrc path.
    - Investigated the exact configurations and the audio features that were being
    used by opensmile.
        - In roddy's code, he is using eGemapsv01a --> We not use eGemapsv02.
        - To use opensmile, all the Gemaps and eGemaps config files have to be
            copied, along with the shared files.
            See link: https://audeering.github.io/opensmile/get-started.html#default-feature-sets


DATE: 5/15/22
- Developing notebooks for extracting and understanding features:
    - NOTE: This notebook is specifically trying to obtain features that were
        used in the original Skantze paper.
    - NOTE: This is in the notebooks directory.
- Created methods for extracting voice activity annotations and pitch features.
- TODO:
    - Still need to investigate what the z-normalized pooled features are and
    why they are being used.

DATE: 5/16/22
- Continuing work on skantze_pipeline_poc notebook.
    - Previously had developed methods for extracting:
        1. Voice activity annotations.
        2. Extracting features using Opensmile.
        3. Creating a Pitch feature dataframe using opensmile feature + voice
            activity annotations.
    - Planning on working on the following today:
        - Obtaining spectral stability.
        - Obtaining the parts of speech annotations.
    - Progress today:
        1. Implemented method for extracting voice activity annotations.
            - Annotations for each frame are obtained.
            - The start and end times of a word are used to determine how many
                frames it spans.
        2. Implemented method for extracting Pitch
            - In the original paper, a single measure of pitch is used.
            - Since I have gemaps available through opensmile, I used
                F0semitoneFrom27.5Hz_sma3nz as a pitch feature.
        3. Implemented method for extracting intensity.
            - For gemaps, this was "Loudness_sma3", extracted similarly to Pitch.
        4. Implemented method for extracting delayed POS annotations.
            - This took a while to implement.
            - The pos tags are annotated in .pos files but these do not have start
                or end time annotations. Instead, they reference the timed-unit
                files. Therefore, I had to read both files and match the ids.
            - There are 59 POS tags annotated in MapTask.
            - First, I extract the end times and pos annotations from MapTask.
            - Next I create a mapping from POS tag to a unique integer.
            - Then I use the sklearn one hot encoder, fit it on the POS tags mapping,
                and encode each POS tag as a 59 feature vector.
            - To introduce delay, find the closest frame to word end time + DELAY_TIME
                and add the POS tag there.
            - All frames for which there are no tags are all zeros.

    - Next TODO:
        - Implement the Spectral Stability methods
        - Create scripts to extract these features for all files.
- Resources found:
    - PyTorch audio also provides methods for extracting / transforming
    individual audio features, which may be useful later.
        https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#pitch


DATE: 5/17/22

- Goals for today:
    1. Continue to develop the pipeline for the skantze 2017 paper in the
        skantze_pipeline_poc notebook. --> Extract spectral stability.
    2. Clean up the existing Maptask folders to separate the different pipelines.
    3. Create methods to generate results for all data files - Skantze 2018 pipeline.

- Progress today:
    1. Extracting Spectral Stability - Skantze 2017 pipeline.
        - Can't use the same method as in the original paper - We are not
        getting the power spectrum divided in N bands.
        - Instead, we use Spectral Flux from the GeMAP features, which seems to
        be the most similar metric.
        - Fixing the notebooks to make sure that the individual feature
        functions are standalone.
    2. Clean up the existing Maptask folders to separate the different pipelines.
        - Creating a separate download directory with the download script.
        - Created a script to download the maptask corpus.
        - Created a script to extract features from audio file directory.

- TODO:
    1. Extract gemaps for all the audio data.
    2. Refactor the Skantze 2017 POC notebook to use a single version of the data.
    2. Create a notebook to extract the skantze features for all the maptask files.

DATE: 5/18/22
- Goals for today:
    1. Write a script based pipeline to extract all the features used for the
        skantze 2017 paper.
    2. Construct training, validation, and test set from the extracted features.
    3. Develop algorithms for creating test sets for TRP prediction.

- Progress today:
    1. Wrote a pipeline to extract features as required by 2017 skantze.
        - This script can generate both the full and prosody feature sets.
        - The script takes methods from the skantze_pipeline_poc notebook.
        - Note that the pipeline can only generate features for 50ms framesize.
            - Issues with generating for 10ms frames:
                1. Features from opensmile are 50ms frame size.
                2. Not sure how to shift the features for 10ms as in Roddy
                    - Not even sure if this is valid.

- TODO:
    1. Construct training, validation, and test set from the extracted features.
    2. Develop algorithms for creating test sets for TRP prediction.



