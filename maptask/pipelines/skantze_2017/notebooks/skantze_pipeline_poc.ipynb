{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About \n",
    "\n",
    "This notebooks is aimed at obtaining different audio features from audio files \n",
    "(including the GeMAPSv01b features set). It is intended to be a proof of concept \n",
    "for how these features may be extracted. Additionally, it investigates the \n",
    "results generated from the refactored data pipeline.\n",
    "\n",
    "\n",
    "NOTE: From the original paper, we need the following features for our data vectors for the LSTM model:\n",
    "1. Voice Activity\n",
    "2. Pitch \n",
    "3. Spectral Stability \n",
    "4. Parts of Speech \n",
    "\n",
    "Additional feature investigation uses the following features:\n",
    "1. Acoustic features:\n",
    "    - eGeMaps\n",
    "2. Linguistic Features:\n",
    "    - Parts of Speech \n",
    "    - Word tags. \n",
    "3. Phonetic Features \n",
    "    - Senone bottleneck features. \n",
    "4. Voice Activity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE 5/17/22**\n",
    "\n",
    "This notebook was used to develop initial methods for feature extraction for the Skantze 2017 paper. For a notebook containing the final pipeline, please \n",
    "search for skantze_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from glob import glob \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths\n",
    "# Dir paths.\n",
    "PROJECT_ROOT_PATH = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_PATH,\"data\")\n",
    "MAPTASK_PATH = os.path.join(DATA_PATH,\"maptaskv2-1\")\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT_PATH,\"results\")\n",
    "STEREO_AUDIO_PATH = os.path.join(MAPTASK_PATH,\"Data/signals/dialogues\")\n",
    "MONO_AUDIO_PATH = os.path.join(MAPTASK_PATH,\"Data/signals/mono_dialogues\")\n",
    "# NOTE: The timed units are also used for Voice Activity annotations. \n",
    "TIMED_UNIT_PATHS = os.path.join(MAPTASK_PATH,\"Data/timed-units\") \n",
    "POS_PATH = os.path.join(MAPTASK_PATH,\"Data/pos\")\n",
    "GEMAPS_DIR = os.path.join(DATA_PATH,\"gemaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use very specific data files since the goal for this notebook is to investigate \n",
    "how different features of the pipeline are working and what values they produce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIALOGUE_NAMES_SPLIT = [\"q1ec1\"]\n",
    "PARTICIPANT_LABELS_MAPTASK = [\"f\",\"g\"] # NOTE: f = follower ; g = giver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some utility functions and their tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timed_unit(dialogue_name,participant):\n",
    "    timed_unit_paths = [os.path.join(TIMED_UNIT_PATHS,p) for p in os.listdir(TIMED_UNIT_PATHS)]\n",
    "    for path in timed_unit_paths:\n",
    "        basename = os.path.basename(path)\n",
    "        timed_participant = basename[basename.find(\".\")+1:basename.find(\".\")+2]\n",
    "        if basename[:basename.find(\".\")] == dialogue_name and\\\n",
    "                timed_participant == participant:\n",
    "            return path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mono_audio(dialogue_name, participant):\n",
    "    audio_paths = [os.path.join(MONO_AUDIO_PATH,p) for p in os.listdir(MONO_AUDIO_PATH)]\n",
    "    for path in audio_paths:\n",
    "        basename = os.path.basename(path)\n",
    "        audio_participant = basename[basename.find(\".\")+1:basename.find(\".\")+2]\n",
    "        if basename[:basename.find(\".\")] == dialogue_name and\\\n",
    "                audio_participant == participant:\n",
    "            return path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stereo_audio(dialogue_name):\n",
    "    audio_paths = [os.path.join(STEREO_AUDIO_PATH,p) for p in os.listdir(STEREO_AUDIO_PATH)]\n",
    "    for path in audio_paths:\n",
    "        basename = os.path.basename(path)\n",
    "        if basename[:basename.find(\".\")] == dialogue_name:\n",
    "            return path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir_path,dialogue_name, participant,ext):\n",
    "    \"\"\"\n",
    "    Assumption is that the basename . is the dialogue name. \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    data_paths = [p for p in os.listdir(dir_path)]\n",
    "    data_paths = [os.path.join(dir_path,p) for p in data_paths if os.path.splitext(p)[1][1:] == ext]\n",
    "    for path in data_paths:\n",
    "        basename = os.path.basename(path)\n",
    "        audio_participant = basename[basename.find(\".\")+1:basename.find(\".\")+2]\n",
    "        if basename[:basename.find(\".\")] == dialogue_name and\\\n",
    "                audio_participant == participant:\n",
    "            results.append(path)\n",
    "    return results \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/maptaskv2-1/Data/timed-units/q1ec1.g.timed-units.xml'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_timed_unit(DIALOGUE_NAMES_SPLIT[0],\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/maptaskv2-1/Data/signals/mono_dialogues/q1ec1.f.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mono_audio(DIALOGUE_NAMES_SPLIT[0],\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/maptaskv2-1/Data/signals/dialogues/q1ec1.mix.wav'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stereo_audio(DIALOGUE_NAMES_SPLIT[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/gemaps/q1ec1.f.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data(GEMAPS_DIR,DIALOGUE_NAMES_SPLIT[0], \"f\",\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Skantze 2017 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in this section is to extract all the features that are required for the original LSTM model. \n",
    "\n",
    "These features include:\n",
    "1. Voice Activity --> From dataset annotations \n",
    "2. Pitch --> From opensmile\n",
    "3. Spectral Stability --> Not sure \n",
    "4. Parts of Speech --> Annotations supplied with the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_STEP_MS = 50 # In the original paper, features were extracted every 50 ms. \n",
    "FRAME_SIZE_MS = 50 # In the original paper, each frame was 50 ms long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Activity\n",
    "\n",
    "The voice activity annotations can be directly extracted from the MapTask corpus timed-units, which have tu tags for when there was an utterance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum utterance duration for it to be considered voice activity. \n",
    "MINIMUM_VA_CLASSIFICATION_TIME_MS = 25 \n",
    "VOICE_ACTIVITY_LABEL = 1 # This means that voice activity was detected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: This voice activity is for 50ms intervals. \n",
    "def get_voice_activity_annotations(dialogue_name, participant):\n",
    "    timed_unit_path = get_timed_unit(dialogue_name,participant)\n",
    "    # Read the xml file \n",
    "    tree = xml.etree.ElementTree.parse(timed_unit_path).getroot()\n",
    "    # Extracting the audio end time from te timed units file. \n",
    "    audio_end_time_ms = float(list(tree.iter())[-1].get('end')) *1000\n",
    "    tu_tags = tree.findall('tu')\n",
    "    # Getting all the times in which there are voice activity annotations in the corpus. \n",
    "    va_times = []\n",
    "    for tu_tag in tu_tags:\n",
    "        start_time_s = float(tu_tag.get('start'))\n",
    "        end_time_s = float(tu_tag.get('end'))\n",
    "        if end_time_s - start_time_s >= MINIMUM_VA_CLASSIFICATION_TIME_MS/1000:\n",
    "            va_times.append((start_time_s,end_time_s))\n",
    "    # Get the frame times based on the final times unit time. \n",
    "    # NOTE: This is being generated based on the step size for now. \n",
    "    frame_times_s = np.arange(0,audio_end_time_ms,FRAME_STEP_MS) / 1000\n",
    "    # Array to store voice  activity - initially all zeros means no voice activity. \n",
    "    voice_activity = np.zeros((frame_times_s.shape[0]))\n",
    "    # For each activity detected, get the start and end index of the nearest frame being \n",
    "    # considered from the input audio. \n",
    "    for start_time_s, end_time_s in va_times:\n",
    "        # Obtaining index relative to the frameTimes being considered for \n",
    "        # which there is voice activity. \n",
    "        start_idx = np.abs(frame_times_s-start_time_s).argmin()\n",
    "        end_idx = np.abs(frame_times_s-end_time_s).argmin()\n",
    "        voice_activity[start_idx:end_idx+1] = VOICE_ACTIVITY_LABEL\n",
    "    return pd.DataFrame({\n",
    "        \"frameTime\" : frame_times_s,\n",
    "        \"voiceActivity\" :  voice_activity\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting VA annotations \n",
    "voice_activity = get_voice_activity_annotations(\n",
    "    DIALOGUE_NAMES_SPLIT[0],PARTICIPANT_LABELS_MAPTASK[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_activity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(np.where(voice_activity == 0)).shape, np.array(np.where(voice_activity == VOICE_ACTIVITY_LABEL)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Percentage of frames with voice activity \n",
    "(np.array(np.where(voice_activity == VOICE_ACTIVITY_LABEL)).shape[1] / \\\n",
    "np.array(np.where(voice_activity == 0)).shape[1]) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the voice activity for both f and g participants. \n",
    "voice_activity_f_df = get_voice_activity_annotations(\n",
    "    DIALOGUE_NAMES_SPLIT[0],\"f\")\n",
    "voice_activity_g_df = get_voice_activity_annotations(\n",
    "    DIALOGUE_NAMES_SPLIT[0],\"g\")\n",
    "voice_activity_f_df.to_csv(\"{}/{}.f.voice_activity.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))\n",
    "voice_activity_g_df.to_csv(\"{}/{}.g.voice_activity.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch \n",
    "\n",
    "We use the [eGeMAPS](https://sail.usc.edu/publications/files/eyben-preprinttaffc-2015.pdf) feature set in which frequency related features include Pitch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features in eGeMAPS, which can be extracted using [OpenSmile](https://audeering.github.io/opensmile/get-started.html#default-feature-sets) are defined [here](https://link.springer.com/content/pdf/bbm%3A978-3-319-27299-3%2F1.pdf). They are separated into individual \n",
    "categories as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: These categories are defined in the original paper. \n",
    "# NOTE: There are other features extracted by OpenSmile but they were not defined \n",
    "# in the original paper - and are not used here - but these might be useful later. \n",
    "# Ex. Mfccs. \n",
    "# The names have been adapted for use with the results produced by opensmile. \n",
    "\n",
    "\n",
    "GEMAPS_FREQUENCY_FEATURES = [\n",
    "    'F0semitoneFrom27.5Hz_sma3nz', # Pitch: logarithmic F0 on a semitone frequency scale, starting at 27.5 Hz (semitone 0)\n",
    "    \"jitterLocal_sma3nz\", # Jitter, deviations in individual consecutive F0 period lengths.\n",
    "     # Formant 1, 2, and 3 frequency, centre frequency of first, second, and third formant\n",
    "    \"F1frequency_sma3nz\",\n",
    "    \"F2frequency_sma3nz\", \n",
    "    \"F3frequency_sma3nz\", \n",
    "    \"F1bandwidth_sma3nz\"\n",
    "] \n",
    "    \n",
    "GEMAPS_ENERGY_FEATURES = [\n",
    "    \"shimmerLocaldB_sma3nz\", # Shimmer, difference of the peak amplitudes of consecutive F0 periods.\n",
    "    \"Loudness_sma3\", # Loudness, estimate of perceived signal intensity from an auditory spectrum.\n",
    "    \"HNRdBACF_sma3nz\" # Harmonics-to-Noise Ratio (HNR), relation of energy in harmonic components to energy in noiselike components.\n",
    "]\n",
    "\n",
    "GEMAPS_SPECTRAL_FEATURES = [\n",
    "    \"alphaRatio_sma3\", #  Alpha Ratio, ratio of the summed energy from 50–1000 Hz and 1–5 kHz\n",
    "    \"hammarbergIndex_sma3\",  # Hammarberg Index, ratio of the strongest energy peak in the 0–2 kHz region to the strongest peak in the 2–5 kHz region\n",
    "    # Spectral Slope 0–500 Hz and 500–1500 Hz, linear regression slope of the logarithmic power spectrum within the two given bands\n",
    "    \"slope0-500_sma3\", \n",
    "    \"slope500-1500_sma3\", \n",
    "    # Formant 1, 2, and 3 relative energy, as well as the ratio of the energy of the spectral harmonic\n",
    "    # peak at the first, second, third formant’s centre frequency to the energy of the spectral peak at F0.\n",
    "    \"F1amplitudeLogRelF0_sma3nz\", \n",
    "    \"F2amplitudeLogRelF0_sma3nz\", \n",
    "    \"F3amplitudeLogRelF0_sma3nz\", \n",
    "    \"logRelF0-H1-H2_sma3nz\", # Harmonic difference H1–H2, ratio of energy of the first F0 harmonic (H1) to the energy of the second F0 harmonic (H2)\n",
    "    \"logRelF0-H1-A3_sma3nz\" # Harmonic difference H1–A3, ratio of energy of the first F0 harmonic (H1) to the energy of the highest harmonic in the third formant range (A3).\n",
    "]\n",
    "\n",
    "# These are all the GeMAPS features we are interested in. \n",
    "RELEVANT_GEMAP_FEATURES = GEMAPS_FREQUENCY_FEATURES + GEMAPS_ENERGY_FEATURES + \\\n",
    "    GEMAPS_SPECTRAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMAPS_CSV_DELIMITER = \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the frameTimes \n",
    "# NOTE: Not sure why the original code is doing this. \n",
    "gemaps_path = read_data(GEMAPS_DIR,DIALOGUE_NAMES_SPLIT[0], \"g\",\"csv\")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: In the code below, we assume that the extracted opensmile features \n",
    "were extracted with the same frame step and frame size as defined for this notebook. \n",
    "\n",
    "If the timescale we are interested in is smaller than the timescale that was \n",
    "used to extract the features, then we should shift the features **back** by some \n",
    "amount. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PITCH_FEATURE_LABELS = \"F0semitoneFrom27.5Hz_sma3nz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the amount by which the opensmile features are shifted back\n",
    "# NOTE: This is because we are using a 50ms timestep and the extracted features \n",
    "# are also on a 50 ms timescale. \n",
    "\n",
    "# Read the gemaps feature file. \n",
    "gemaps_df = pd.read_csv(gemaps_path,delimiter=GEMAPS_CSV_DELIMITER)\n",
    "# Extract the relevant raw gemap features into a separate file. \n",
    "relevant_gemaps_df = gemaps_df[RELEVANT_GEMAP_FEATURES]\n",
    "# Obtain the z normalized values for each column individually. \n",
    "z_normalized_feat_df = relevant_gemaps_df.apply(\n",
    "    lambda col: preprocessing.scale(col),axis=1,result_type='broadcast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the available GeMAPS features extracted from opensmile. \n",
    "\n",
    "# Read the gemaps feature file. \n",
    "gemaps_df = pd.read_csv(gemaps_path,delimiter=GEMAPS_CSV_DELIMITER)\n",
    "gemaps_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original paper uses both the absolute and relevant pitch values and a \n",
    "# binary label indicating whether the frame was voiced. \n",
    "absolute_pitch = relevant_gemaps_df[PITCH_FEATURE_LABELS]\n",
    "z_normalized_pitch = z_normalized_feat_df[PITCH_FEATURE_LABELS]\n",
    "# Determine whether frame was voiced \n",
    "frame_times_s = gemaps_df[\"frameTime\"] \n",
    "z_normalized_pitch\n",
    "data = {\n",
    "    \"frameTime\" : frame_times_s,\n",
    "    \"{}Absolute\".format(PITCH_FEATURE_LABELS) : absolute_pitch, \n",
    "    \"{}Znormelized\".format(PITCH_FEATURE_LABELS) : z_normalized_pitch, \n",
    "    \"voiceActivity\" : voice_activity[\"voiceActivity\"]\n",
    "}\n",
    "pitch_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_df.to_csv(\"{}/{}.g.pitch.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a method for extraction. \n",
    "\n",
    "def extract_pitch(dialogue_name, participant, va_annotation_df):\n",
    "    # Define the amount by which the opensmile features are shifted back\n",
    "    # NOTE: This is because we are using a 50ms timestep and the extracted features \n",
    "    # are also on a 50 ms timescale. \n",
    "\n",
    "    # Read the gemaps feature file. \n",
    "    gemaps_path = read_data(GEMAPS_DIR,dialogue_name, participant,\"csv\")[0]\n",
    "    gemaps_df = pd.read_csv(gemaps_path,delimiter=GEMAPS_CSV_DELIMITER)\n",
    "    # Extract the relevant raw gemap features into a separate file. \n",
    "    relevant_gemaps_df = gemaps_df[RELEVANT_GEMAP_FEATURES]\n",
    "    # Obtain the z normalized values for each column individually. \n",
    "    z_normalized_feat_df = relevant_gemaps_df.apply(\n",
    "        lambda col: preprocessing.scale(col),axis=1,result_type='broadcast')\n",
    "    # The original paper uses both the absolute and relevant pitch values and a \n",
    "    # binary label indicating whether the frame was voiced. \n",
    "    absolute_pitch = relevant_gemaps_df[PITCH_FEATURE_LABELS]\n",
    "    z_normalized_pitch = z_normalized_feat_df[PITCH_FEATURE_LABELS]\n",
    "    # Determine whether frame was voiced \n",
    "    frame_times_s = gemaps_df[\"frameTime\"] \n",
    "    z_normalized_pitch\n",
    "    data = {\n",
    "        \"frameTime\" : frame_times_s,\n",
    "        \"{}_Absolute\".format(PITCH_FEATURE_LABELS) : absolute_pitch, \n",
    "        \"{}_Znormelized\".format(PITCH_FEATURE_LABELS) : z_normalized_pitch, \n",
    "        \"voiceActivity\" : va_annotation_df[\"voiceActivity\"]\n",
    "    }\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for both speakers in one file \n",
    "pitch_f_df = extract_pitch(DIALOGUE_NAMES_SPLIT[0], \"f\", voice_activity_f_df)\n",
    "pitch_g_df = extract_pitch(DIALOGUE_NAMES_SPLIT[0], \"g\", voice_activity_g_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_f_df.to_csv(\"{}/{}.f.pitch.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))\n",
    "pitch_g_df.to_csv(\"{}/{}.g.pitch.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power / Intensity \n",
    "\n",
    "For now, we consider Loudness from the GeMAPS feature set as a measure of \n",
    "intensity. However, there are other energy related features that may be used \n",
    "later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWER_FEATURE_LABELS = \"Loudness_sma3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_power(dialogue_name, participant):\n",
    "    # Define the amount by which the opensmile features are shifted back\n",
    "    # NOTE: This is because we are using a 50ms timestep and the extracted features \n",
    "    # are also on a 50 ms timescale. \n",
    "    # Read the gemaps feature file. \n",
    "    gemaps_path = read_data(GEMAPS_DIR,dialogue_name, participant,\"csv\")[0]\n",
    "    gemaps_df = pd.read_csv(gemaps_path,delimiter=GEMAPS_CSV_DELIMITER)\n",
    "    # Extract the relevant raw gemap features into a separate file. \n",
    "    relevant_gemaps_df = gemaps_df[RELEVANT_GEMAP_FEATURES]\n",
    "    # Obtain the z normalized values for each column individually. \n",
    "    z_normalized_feat_df = relevant_gemaps_df.apply(\n",
    "        lambda col: preprocessing.scale(col),axis=1,result_type='broadcast')\n",
    "    # The original paper uses power / intensity in dB - \n",
    "    # TODO: Check what the units of loudness are in the GeMAPS set. \n",
    "    absolute_power = relevant_gemaps_df[POWER_FEATURE_LABELS]\n",
    "    z_normalized_power = z_normalized_feat_df[POWER_FEATURE_LABELS]\n",
    "    # Determine whether frame was voiced \n",
    "    frame_times_s = gemaps_df[\"frameTime\"] \n",
    "    data = {\n",
    "        \"frameTime\" : frame_times_s,\n",
    "        \"{}_Absolute\".format(POWER_FEATURE_LABELS) :  absolute_power,\n",
    "        \"{}_Znormalized\".format(POWER_FEATURE_LABELS) : z_normalized_power\n",
    "    }\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_f_df = extract_power(DIALOGUE_NAMES_SPLIT[0], \"f\")\n",
    "power_g_df = extract_power(DIALOGUE_NAMES_SPLIT[0], \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "power_f_df.to_csv(\"{}/{}.f.power.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))\n",
    "power_g_df.to_csv(\"{}/{}.g.power.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Stability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral stability in the original paper is a derived measure. First, \n",
    "Snack FFT analysis was used to get the power spectrum divided into N bands \n",
    "(up to 4kHz) at each time step. The equation in the skantze paper was used \n",
    "to calculate the Stability at time t. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTRAL_FEATURE_LABELS = 'spectralFlux_sma3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_flux(dialogue_name, participant):\n",
    "    # Define the amount by which the opensmile features are shifted back\n",
    "    # NOTE: This is because we are using a 50ms timestep and the extracted features \n",
    "    # are also on a 50 ms timescale. \n",
    "    # Read the gemaps feature file. \n",
    "    gemaps_path = read_data(GEMAPS_DIR,dialogue_name, participant,\"csv\")[0]\n",
    "    gemaps_df = pd.read_csv(gemaps_path,delimiter=GEMAPS_CSV_DELIMITER)\n",
    "    # Extract the relevant raw gemap features into a separate file. \n",
    "    spectral_flux_df = gemaps_df[SPECTRAL_FEATURE_LABELS]\n",
    "    # Obtain the z normalized values for each column individually. \n",
    "    z_normalized_spectral_flux_df = preprocessing.scale(spectral_flux_df)\n",
    "    # Determine whether frame was voiced \n",
    "    frame_times_s = gemaps_df[\"frameTime\"] \n",
    "    data = {\n",
    "        \"frameTime\" : frame_times_s,\n",
    "        \"{}_Znormalized\".format(SPECTRAL_FEATURE_LABELS) : z_normalized_spectral_flux_df \n",
    "    }\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_flux_f_df = extract_spectral_flux(DIALOGUE_NAMES_SPLIT[0], \"f\")\n",
    "spectral_flux_g_df = extract_spectral_flux(DIALOGUE_NAMES_SPLIT[0], \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "spectral_flux_f_df .to_csv(\"{}/{}.f.spectral_flux.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))\n",
    "spectral_flux_g_df .to_csv(\"{}/{}.g.spectral_flux.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Annotations\n",
    "\n",
    "These are directly obtained from the annotations received with the MapTask corpus. \n",
    "\n",
    "In the original paper, there are 59 different POS tags, and all the tags are \n",
    "represented as a one hot feature vector. Additionally, to simulate the delay \n",
    "in extracting POS tags, the feature vector was set to 0 by default but the \n",
    "corresponding feature vector was set to 1 (since it is a one-hot encoded vector)\n",
    "a 100ms after the word had ended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to read the timed-unit file for the corresponding start and end times \n",
    "# for the tags since we need to assign it\n",
    "timed_unit_path = read_data(\n",
    "    TIMED_UNIT_PATHS,DIALOGUE_NAMES_SPLIT[0],PARTICIPANT_LABELS_MAPTASK[1],\"xml\")[0]\n",
    "tree_timed_unit = xml.etree.ElementTree.parse(timed_unit_path).getroot()\n",
    "timed_unit_tags = list(tree_timed_unit.iter())\n",
    "tu_tags = tree_timed_unit.findall(\"tu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the appropriate pos file\n",
    "pos_path = read_data(\n",
    "    POS_PATH,DIALOGUE_NAMES_SPLIT[0],PARTICIPANT_LABELS_MAPTASK[1],\"xml\")[0]\n",
    "tree_pos = xml.etree.ElementTree.parse(pos_path).getroot()\n",
    "# The pos is the tag attribute in all the tw tags. \n",
    "tw_tags = tree_pos.findall(\"tw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting all the times in which there are voice activity annotations in the corpus. \n",
    "va_times = []\n",
    "for tu_tag in tu_tags:\n",
    "    start_time_s = float(tu_tag.get('start'))\n",
    "    end_time_s = float(tu_tag.get('end'))\n",
    "    if end_time_s - start_time_s >= MINIMUM_VA_CLASSIFICATION_TIME_MS/1000:\n",
    "        va_times.append((start_time_s,end_time_s))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_DELAY_TIME_MS = 100 # Assume that each POS calculation is delayed by 100ms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the audio end time from te timed units file. \n",
    "audio_end_time_ms = float(list(tree_timed_unit.iter())[-1].get('end')) *1000\n",
    "# Get the frame times based on the final times unit time. \n",
    "# NOTE: This is being generated based on the step size for now. \n",
    "frame_times_s = np.arange(0,audio_end_time_ms,FRAME_STEP_MS) / 1000\n",
    "frame_times_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have a POS annotation per frame - not simply per detected word time. \n",
    "pos_annotations = [0] * frame_times_s.shape[0] \n",
    "len(pos_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the end time of the word and the corresponding POS tag. \n",
    "word_annotations = []\n",
    "for tu_tag in tu_tags:\n",
    "    tu_tag_id = tu_tag.get(\"id\")[7:]\n",
    "    end_time_s = float(tu_tag.get('end'))\n",
    "    for tw_tag in tw_tags:\n",
    "        # NOTE: Not sure if this is the correct way to extract the corresponding \n",
    "        # timed-unit id. \n",
    "        href = list(tw_tag.iter())[1].get(\"href\")\n",
    "        href_filename, href_ids = href.split(\"#\")\n",
    "        # Look at the appropriate file tags based on the filename. \n",
    "        href_ids = href_ids.split(\"..\")\n",
    "        for href_id in href_ids:\n",
    "            href_id = href_id[href_id.find(\"(\")+8:href_id.rfind(\")\")]\n",
    "            if href_id == tu_tag_id:\n",
    "                word_annotations.append((end_time_s,tw_tag.get(\"tag\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating the one hot encodings from an array representing the \n",
    "POS annotation for each frame (delayed X ms after the end of the word). \n",
    "\n",
    "\n",
    "Note that 0 represents that no annotation was available for that time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from all the POS annotation tags\n",
    "# Documentation to the MapTask POS tags:  https://groups.inf.ed.ac.uk/maptask/interface/expl.html\n",
    "POS_TAGS = [\n",
    "    \"vb\", \n",
    "    \"vbd\", \n",
    "    \"vbg\",\n",
    "    \"vbn\", \n",
    "    \"vbz\",\n",
    "    \"nn\",\n",
    "    \"nns\",\n",
    "    \"np\",\n",
    "    \"jj\",\n",
    "    \"jjr\",\n",
    "    \"jjt\",\n",
    "    \"ql\",\n",
    "    \"qldt\",\n",
    "    \"qlp\",\n",
    "    \"rb\",\n",
    "    \"rbr\",\n",
    "    \"wql\",\n",
    "    \"wrb\",\n",
    "    \"not\",\n",
    "    \"to\",\n",
    "    \"be\",\n",
    "    \"bem\",\n",
    "    \"ber\",\n",
    "    \"bez\",\n",
    "    \"do\",\n",
    "    \"doz\",\n",
    "    \"hv\",\n",
    "    \"hvz\",\n",
    "    \"md\",\n",
    "    \"dpr\",\n",
    "    \"at\",\n",
    "    \"dt\",\n",
    "    \"ppg\",\n",
    "    \"wdt\",\n",
    "    \"ap\",\n",
    "    \"cd\",\n",
    "    \"od\",\n",
    "    \"gen\",\n",
    "    \"ex\",\n",
    "    \"pd\",\n",
    "    \"wps\",\n",
    "    \"wpo\",\n",
    "    \"pps\",\n",
    "    \"ppss\",\n",
    "    \"ppo\",\n",
    "    \"ppl\",\n",
    "    \"ppg2\\\"\",\n",
    "    \"pr\",\n",
    "    \"pn\",\n",
    "    \"in\",\n",
    "    \"rp\",\n",
    "    \"cc\",\n",
    "    \"cs\",\n",
    "    \"aff\",\n",
    "    \"fp\",\n",
    "    \"noi\",\n",
    "    \"pau\",\n",
    "    \"frag\",\n",
    "    \"sent\"\n",
    "]\n",
    "len(POS_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_to_idx = {}\n",
    "idx_to_pos_tag = {}\n",
    "# NOTE: Indices start from 1 here because 0 already represents unknown categories. \n",
    "for i,tag in enumerate(POS_TAGS):\n",
    "    pos_tags_to_idx[tag] = i +1\n",
    "    idx_to_pos_tag[i+1] = tag \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the collected word end times and POS tags, we need to introduce \n",
    "# a delay and add the POS annotation to the delayed frame. \n",
    "pos_annotations = np.zeros((frame_times_s.shape[0]))\n",
    "for end_time_s, pos_tag in word_annotations:\n",
    "    frame_idx = np.abs(frame_times_s-(end_time_s +POS_DELAY_TIME_MS/1000)).argmin()\n",
    "    pos_annotations[frame_idx] = pos_tags_to_idx[pos_tag] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This encoder will ignore any unknown tags by replacing them with all zeros. \n",
    "onehot_encoder = OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\n",
    "onehot_encoder.fit(np.asarray(list(pos_tags_to_idx.values())).reshape(-1,1))\n",
    "onehot_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder.categories_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_pos = onehot_encoder.transform(pos_annotations.reshape(-1,1))\n",
    "encoded_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the extracted features and saving. \n",
    "pos_annotations_df = pd.DataFrame(encoded_pos,columns=POS_TAGS)\n",
    "pos_annotations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinging the above individual cells to create a single method to extract POS \n",
    "# features \n",
    "\n",
    "def extract_pos_annotations_with_delay(dialogue_name, participant):\n",
    "    # Need to read the timed-unit file for the corresponding start and end times \n",
    "    # for the tags since we need to assign it\n",
    "    timed_unit_path = read_data(\n",
    "        TIMED_UNIT_PATHS,dialogue_name,participant,\"xml\")[0]\n",
    "    tree_timed_unit = xml.etree.ElementTree.parse(timed_unit_path).getroot()\n",
    "    tu_tags = tree_timed_unit.findall(\"tu\")\n",
    "    # Read the appropriate pos file\n",
    "    pos_path = read_data(POS_PATH,dialogue_name,participant,\"xml\")[0]\n",
    "    tree_pos = xml.etree.ElementTree.parse(pos_path).getroot()\n",
    "    # The pos is the tag attribute in all the tw tags. \n",
    "    tw_tags = tree_pos.findall(\"tw\")\n",
    "    # Getting all the times in which there are voice activity annotations in the corpus. \n",
    "    va_times = []\n",
    "    for tu_tag in tu_tags:\n",
    "        start_time_s = float(tu_tag.get('start'))\n",
    "        end_time_s = float(tu_tag.get('end'))\n",
    "        if end_time_s - start_time_s >= MINIMUM_VA_CLASSIFICATION_TIME_MS/1000:\n",
    "            va_times.append((start_time_s,end_time_s))\n",
    "    # Extracting the audio end time from te timed units file. \n",
    "    audio_end_time_ms = float(list(tree_timed_unit.iter())[-1].get('end')) *1000\n",
    "    # Get the frame times based on the final times unit time. \n",
    "    # NOTE: This is being generated based on the step size for now. \n",
    "    frame_times_s = np.arange(0,audio_end_time_ms,FRAME_STEP_MS) / 1000\n",
    "    # Collecting the end time of the word and the corresponding POS tag. \n",
    "    word_annotations = []\n",
    "    for tu_tag in tu_tags:\n",
    "        tu_tag_id = tu_tag.get(\"id\")[7:]\n",
    "        end_time_s = float(tu_tag.get('end'))\n",
    "        for tw_tag in tw_tags:\n",
    "            # NOTE: Not sure if this is the correct way to extract the corresponding \n",
    "            # timed-unit id. \n",
    "            href = list(tw_tag.iter())[1].get(\"href\")\n",
    "            href_filename, href_ids = href.split(\"#\")\n",
    "            # Look at the appropriate file tags based on the filename. \n",
    "            href_ids = href_ids.split(\"..\")\n",
    "            for href_id in href_ids:\n",
    "                href_id = href_id[href_id.find(\"(\")+8:href_id.rfind(\")\")]\n",
    "                if href_id == tu_tag_id:\n",
    "                    if tw_tag.get(\"tag\") in POS_TAGS:\n",
    "                        word_annotations.append((end_time_s,tw_tag.get(\"tag\")))\n",
    "    # For all the collected word end times and POS tags, we need to introduce \n",
    "    # a delay and add the POS annotation to the delayed frame. \n",
    "    pos_annotations = np.zeros((frame_times_s.shape[0]))\n",
    "    \n",
    "    for end_time_s, pos_tag in word_annotations:\n",
    "        frame_idx = np.abs(frame_times_s-(end_time_s +POS_DELAY_TIME_MS/1000)).argmin()\n",
    "        pos_annotations[frame_idx] = pos_tags_to_idx[pos_tag] \n",
    "    # This encoder will ignore any unknown tags by replacing them with all zeros. \n",
    "    onehot_encoder = OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\n",
    "    onehot_encoder.fit(np.asarray(list(pos_tags_to_idx.values())).reshape(-1,1))\n",
    "    encoded_pos = onehot_encoder.transform(pos_annotations.reshape(-1,1))\n",
    "    pos_annotations_df = pd.DataFrame(encoded_pos,columns=POS_TAGS)\n",
    "    # Add frametimes to the df \n",
    "    pos_annotations_df.insert(0,\"frameTime\",frame_times_s)\n",
    "    return pos_annotations_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_annotations_f_df = extract_pos_annotations_with_delay(\n",
    "    DIALOGUE_NAMES_SPLIT[0], \"f\")\n",
    "pos_annotations_g_df = extract_pos_annotations_with_delay(\n",
    "    DIALOGUE_NAMES_SPLIT[0], \"g\")\n",
    "pos_annotations_f_df.to_csv(\"{}/{}.f.pos_onehot.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))\n",
    "pos_annotations_g_df.to_csv(\"{}/{}.g.pos_onehot.csv\".format(RESULTS_PATH,DIALOGUE_NAMES_SPLIT[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94eade7d74e86833f221ca63094e1c7cd59b525de511aaf5da72afef04f5d2b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
