{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to download and manipulate data from the Candor \n",
    "corpus for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "\n",
    "# In this mode, the notebook will use small datasets, models, etc. to speed \n",
    "# up training = should be used primarily when running locally. \n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if IS_COLAB:\n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/drive\")\n",
    "\n",
    "import sklearn \n",
    "assert sklearn.__version__ >= \"0.20\" \n",
    "# TensorFlow â‰¥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# Others \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Defining all global paths that will be used in the notebook. \n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CANDOR_URL_FILE = os.path.join(PROJECT_ROOT_DIR,\"file_urls.txt\")\n",
    "DATASET_DIR = os.path.join(PROJECT_ROOT_DIR,\"datasets\")\n",
    "DOWNLOAD_DIR = os.path.join(DATASET_DIR,\"download\")\n",
    "EXTRACT_DIR = os.path.join(DATASET_DIR,\"extract\")\n",
    "DATASET_NAME = \"candor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the relevant directories\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the urls from the url file. \n",
    "with open(CANDOR_URL_FILE,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = lines[0].split(\"https\")\n",
    "    urls = [\"https\" + line.strip() for line in lines if len(line) > 0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://betterup-public-dataset-release.s3.us-west-2.amazonaws.com/v1.0/raw_media_part_001.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIASDTHSKXDJT5YJ2DE%2F20220323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220323T201656Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=b3dac17fc5735cd12c95beae2c68aa476dfd7485c00b8c198c9c7419e3a6cff2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil \n",
    "from tqdm.auto import *\n",
    "import requests\n",
    "import glob \n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "def download_dataset_from_urls(urls, dataset_name,download_dir, extract_dir, unzip=True, chunkSize=8192):\n",
    "    # Create paths \n",
    "    dataset_download_path = os.path.join(download_dir,dataset_name)\n",
    "    dataset_extract_path = os.path.join(extract_dir,dataset_name)\n",
    "    if os.path.isdir(dataset_download_path):\n",
    "        shutil.rmtree(dataset_download_path)\n",
    "    if os.path.isdir(dataset_extract_path):\n",
    "        shutil.rmtree(dataset_extract_path)\n",
    "    os.makedirs(dataset_download_path)\n",
    "    os.makedirs(dataset_extract_path)\n",
    "    # Download each url as a zip file. \n",
    "    print(\"Downloading zip files to folder: {}\".format(dataset_download_path))\n",
    "    if unzip:\n",
    "        print(\"Extracting zip files to folder: {}\".format(dataset_extract_path))\n",
    "    for i,url in enumerate(urls):\n",
    "        # Create a temp. dir for this specific url \n",
    "        name = \"{}_url_{}\".format(dataset_name, i)\n",
    "        url_temp_path = \"{}.zip\".format(os.path.join(dataset_download_path,name))\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            pbar = tqdm(total=int(r.headers['Content-Length']), desc=\"{}\".format(name))\n",
    "            with open(url_temp_path,\"wb+\") as f: \n",
    "                for chunk in r.iter_content(chunk_size=chunkSize):\n",
    "                    if chunk:  # filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "        if unzip:\n",
    "            with ZipFile(url_temp_path, 'r') as zipObj:\n",
    "                # Extract all the contents of zip file in different directory\n",
    "                extract_path = os.path.join(dataset_extract_path,name)\n",
    "                os.makedirs(extract_path)\n",
    "                zipObj.extractall(extract_path)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [urls[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zip files to folder: ./datasets/download/candor\n",
      "Extracting zip files to folder: ./datasets/extract/candor\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://betterup-public-dataset-release.s3.us-west-2.amazonaws.com/v1.0/raw_media_part_001.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIASDTHSKXDJT5YJ2DE%2F20220323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220323T201656Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=b3dac17fc5735cd12c95beae2c68aa476dfd7485c00b8c198c9c7419e3a6cff2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb#ch0000010?line=0'>1</a>\u001b[0m download_dataset_from_urls(test_urls, DATASET_NAME,DOWNLOAD_DIR,EXTRACT_DIR)\n",
      "\u001b[1;32m/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb Cell 9'\u001b[0m in \u001b[0;36mdownload_dataset_from_urls\u001b[0;34m(urls, dataset_name, download_dir, extract_dir, unzip, chunkSize)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb#ch0000008?line=24'>25</a>\u001b[0m url_temp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.zip\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_download_path,name))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb#ch0000008?line=25'>26</a>\u001b[0m \u001b[39mwith\u001b[39;00m requests\u001b[39m.\u001b[39mget(url, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m r:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb#ch0000008?line=26'>27</a>\u001b[0m     r\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb#ch0000008?line=27'>28</a>\u001b[0m     pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(r\u001b[39m.\u001b[39mheaders[\u001b[39m'\u001b[39m\u001b[39mContent-Length\u001b[39m\u001b[39m'\u001b[39m]), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/muhammadumair/Documents/Repositories/local/candor_corpus/candor_corpus.ipynb#ch0000008?line=28'>29</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(url_temp_path,\u001b[39m\"\u001b[39m\u001b[39mwb+\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f: \n",
      "File \u001b[0;32m~/anaconda3/envs/tf_tutorial/lib/python3.8/site-packages/requests/models.py:960\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/muhammadumair/anaconda3/envs/tf_tutorial/lib/python3.8/site-packages/requests/models.py?line=956'>957</a>\u001b[0m     http_error_msg \u001b[39m=\u001b[39m \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code, reason, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl)\n\u001b[1;32m    <a href='file:///Users/muhammadumair/anaconda3/envs/tf_tutorial/lib/python3.8/site-packages/requests/models.py?line=958'>959</a>\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> <a href='file:///Users/muhammadumair/anaconda3/envs/tf_tutorial/lib/python3.8/site-packages/requests/models.py?line=959'>960</a>\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://betterup-public-dataset-release.s3.us-west-2.amazonaws.com/v1.0/raw_media_part_001.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIASDTHSKXDJT5YJ2DE%2F20220323%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220323T201656Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=b3dac17fc5735cd12c95beae2c68aa476dfd7485c00b8c198c9c7419e3a6cff2"
     ]
    }
   ],
   "source": [
    "download_dataset_from_urls(test_urls, DATASET_NAME,DOWNLOAD_DIR,EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "022fc047a21b88219859b15a1aecc2fcf1b7a3629cea36c5e7a56eafcf77c147"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
